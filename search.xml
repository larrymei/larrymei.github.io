<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Particle Filter]]></title>
    <url>%2F2019%2F05%2F13%2FParticle-Filter%2F</url>
    <content type="text"><![CDATA[Particle Filter The key step is shown by this figure. At the beginning of step \(n\), we have samples \(\{z_{n-1}^{(l)}\}\) drawn from \(p(z_{n-1}|x_{1:n-2})\), and the corresponding weights \({w_{n-1}^{(l)}}\), which equals \[ w_{n-1}^{(l)}=\frac{p(x_{n-1}|z_{n-1}^{(l)})}{\sum_{k=1}^{L}{p(x_{n-1}|z_{n-1}^{(k)})}} \] This can be viewed as known $p(z_{n-1}|x_{1:n-1}) $, represented by \[ \begin{align} p(z_{n-1}|x_{1:n-1}) &amp;=p(z_{n-1}|x_{n-1},x_{1:n-2}) \\ &amp;= \frac{p(x_{n-1}|z_{n-1})p(z_{n-1}|x_{1:n-2})}{\int{p(x_{n-1}|z_{n-1})p(z_{n-1}|x_{1:n-2})}dz_{n-1} } \\ &amp;\sim w_{n-1}^{(l)}z_{n-1}^{(l)} \end{align} \] Then, we draw again \(L\) samples from \(\{z_{n-1}^{(l)}\}\) with weights \({w_{n-1}^{(l)}}\) and \(p(z_n|z_{n-1}^{(l)})\) to obtain new samples \(\{z_{n}^{(l)}\} \sim p(z_n|x_{1:n-1})\) . This procedure is called resampling, expressed as \[ \begin{align} p(z_n|x_{1:n-1}) &amp;=\int{p(z_n |x_{1:n-1},z_{n-1})p(z_{n-1}|x_{1:n-1})}dz_n \\ &amp;=\int{p(z_n|z_{n-1})p(z_{n-1}|x_{1:n-1})}dz_n \\ &amp;=\int{p(z_n|z_{n-1})p(z_{n-1}|x_{1:n-1})}dz_n \\ &amp;=\sum_l{w_{n-1}^{(l)}}p(z_n|z_{n-1}^{(l)}) \end{align} \] To this end, the new weights \({w_{n}^{(l)}}\) are calculated by the new observations, \[ w_n^{(l)}=\frac{p(x_n|z_n^{(l)})}{\sum_{k=1}^{L}{p(x_n|z_n^{(k)})}} \] Also, obtaining \[ \begin{align} p(z_n|x_{1:n}) &amp;=p(z_n|x_n,x_{1:n-1}) \\ &amp;= \frac{p(x_n|z_n)p(z_n|x_{1:n-1})}{\int{p(x_n|z_n)p(z_n|x_{1:n-1})}dz_n } \\ &amp;\sim w_n^{(l)}z_n^{(l)} \end{align} \] and we can iterate this process.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Importance sampling]]></title>
    <url>%2F2019%2F05%2F13%2FImportance-sampling%2F</url>
    <content type="text"><![CDATA[Importance Sampling When \(p(z)\) is a complicate distribution and difficult to sample, we can alternatively sample from an much simpler distribution \(q(z)\) to approximate this distribution. Consider the case to calculate the expectation of \(f(z)\) given the distribution \(p(z)\), we can express it in the form of a finite sum over samples \(\{z^{(l)}\}\) drawn from \(q(z)\) \[ \begin{align} \mathbb{E}[f] &amp;= \int{f(z)p(z)dz} \\ &amp;= \int{f(z)\frac{p(z)}{q(z)}q(z)dz} \\ &amp;\approx \frac{1}{L}\sum^L_{l=1}\frac{p(z^{(l)})}{q(z^{(l)})}f(z^{(l)}) \end{align} \] The quantities \(r_l=p(z^{(l)})/q(z^{(l)})\) are known as importance weights, and they correct the bias introduced by sampling from the wrong distribution. It will be often the case that \(p(z)\) will only be evaluated up to a normalization constant, \(p(z)=\tilde{p}(z)/Z_p\) where \(Z_p\) is unknown. \[ \begin{align} \mathbb{E}[f] &amp;= \frac{Z_q}{Z_p}\int{f(z)\frac{\tilde{p}(z)}{\tilde{q}(z)}q(z)dz} \\ &amp;\approx \frac{Z_q}{Z_p}\frac{1}{L}\sum^L_{l=1}\tilde{r_l}f(z^{(l)}) \end{align} \] where \(\tilde{r_l}=\tilde{p}(z^{(l)})/\tilde{q}(z^{(l)})\). The ratio \(Z_q/Z_p\) can also be evaluted by \[ \begin{align} \frac{Z_p}{Z_q}&amp;=\frac{1}{Z_q}\int{\tilde{p}(z)}dz=\int{\frac{\tilde{p}(z)}{\tilde{q}(z)}q(z)dz} \\ &amp;\approx \frac{1}{L}\sum^L_{l=1}\tilde{r_l} \end{align} \] Thus \[ \mathbb{E}[f]\approx\sum_{l=1}^Lw_lf(z^{(l)}) \] where we have defined \[ w_l=\frac{\tilde{r_l}}{\sum_k\tilde{r_k}}=\frac{\tilde{p}(z^{(l)})/\tilde{q}(z^{(l)})}{\sum_k{\tilde{p}(z^{(k)})/\tilde{q}(z^{(k)})}}=\frac{\tilde{p}(z^{(l)})/q(z^{(l)})}{\sum_k{\tilde{p}(z^{(k)})/q(z^{(k)})}} \]]]></content>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression]]></title>
    <url>%2F2019%2F03%2F20%2FLogistic-Regression%2F</url>
    <content type="text"><![CDATA[Logistic Regression \(n_x\): dimension of \(x\) \(m\): number of samples Task Given \(x\) , want to find an estimate: \(\hat{y}=p(y=1|x)\) . \(x\in{\mathbb{R}^{n_x}}\) , \(0 \leq\hat{y}\leq1\) Parameter: \(w\in\mathbb{R}^{n_x}\), \(b\in\mathbb{R}​\) Output: \(\hat{y}=\sigma(w^Tx+b)\) sigmoid function Graph Function Derivative \[\sigma(z)=\cfrac{1}{1+e^{-z}}\] \(\sigma(z)(1-\sigma(z))\) Cost function Given \(\{(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})\}\), want \(\hat{y}^{(i)}\approx y^{(i)}\) Loss function: Lest square loss: \(\mathcal{L}(\hat{y},y)=\frac12(\hat{y}-y)^2\) Logistic loss: \(\mathcal{L}(\hat{y},y)=-(y\log\hat{y}+(1-y)\log(1-\hat{y}))\) Cost function: \(J(w,b)=\frac{1}{m}\sum_{i=1}^{m}\mathcal{L}(\hat{y}^{(i)},y^{(i)})=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log\hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)})]​\) Derivative Vectorization \[ X = \left[ \begin{array}{cccc} \mid &amp; \mid &amp; &amp; \mid \\ x^{(1)} &amp; x^{(2)} &amp; \ldots &amp; x^{(n)} \\ \mid &amp; \mid &amp; &amp; \mid \end{array} \right] \in \mathbb{R}^{(n_x,m)} \] \[ Z=\begin{bmatrix} z^{(1)} &amp; z^{(2)} &amp; \ldots &amp;z^{(n)}\end{bmatrix}=w^TX+\boldsymbol{b} \] Forward Propagation: \[ A = \sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)}) \] \[ J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)}) \] Backward Propagation: \[ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T \] \[ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)}) \]]]></content>
  </entry>
  <entry>
    <title><![CDATA[Deep learning notes]]></title>
    <url>%2F2019%2F02%2F27%2FDeep-learning-notes%2F</url>
    <content type="text"><![CDATA[​ 最近在coursera上Andrew Ng的deep learning系列课程，整个课程干货满满，无论是初学者还是有经验的deep learning从业者都能收获不少。系列课程一共有5门课，每门大约2-4周的课时，依次讲述了标准deep neural network的原理， 经典网络训练优化方法，deep learning项目的实践技巧， convolutional neural network以及recurrent neural network。 ​ 花了一周的时间完成了前三门课程，Andrew对课程的讲述恰到好处，避免了冗杂的公式推导以保证课程讲述的流畅性。课程以实践应用为导向，更多的从intuition的角度出发，在理论上涉及的部分则略少。比如介绍几种网络训练方法SGD, RMSProp, Adam时主要介绍是怎么计算的，而忽略了一些收敛速度的解释。不过瑕不掩瑜，没有一种课程是可以满足所有人的学习要求的，Andrew的这门课程主要是面对工程应用，可以说已经做到近乎极致了。 ​ 另外值得一提的是这门系列课程的课程作业非常的好，一定要从到到尾过一遍。每次的作业会让学生完成一个小的project，比如构建一个logistic regression。project会预先分解为多个函数，每个函数的主体代码已经基本完成，剩下一小部分关键代码由学生补齐。函数前会有大量的文本解释，并且提示所需的数学公式，函数中有大量的注释，对于输入输出也有详尽的说明。最牛逼的是在这些函数后都有函数的测试数据提供，用一些固定的测试数据测试所完成的函数，并提供标准输出作为检验。这相当与把整个project分成了一个个小任务，每次小任务都及时给予反馈。一方面学生可以及时查验自己有没有出错，更厉害的是让学生可以快速获得正向激励，取得成就感。以我个人经验来说，就是爽的飞起根本停不下来。不得不说，Andrew很知道学习的痛点在哪，这套方法让学习变得非常有趣。整个课程作业的设计也非常赞，从浅及深，先是从设计最基本的神经网络开始，辅以大量的数据，让学生完成一个个有趣的机器学习案例，比如识别图像中的猫，分类照片里的手势数字等等。大部分的数据预处理部分都已经提供，学生可以直接接触算法的核心部分。随着课程的深入，神经网络也越来越复杂，课程的后半部分开始介绍使用常用的深度学习框架Tensorflow和Keras。这部分的内容也可以当做学习框架的入门教程。最后值得称赞的是，无论是从规范性上还是技巧性上来说，课程作业的代码质量极高。特别对是初学者，仔细研读代码可以极大的提高自己代码的水平。 ​ 完成课程对我来说没有难度，本来打算一口气刷掉全部5门课，不过仔细一想切勿好高骛远，还是应该暂时停下来整理一下所学的内容，避免忘记。下面是总结笔记。]]></content>
  </entry>
  <entry>
    <title><![CDATA[金融时间序列分析]]></title>
    <url>%2F2019%2F02%2F26%2F%E9%87%91%E8%9E%8D%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1. 时间序列的特征 一个时间序列可能存在的特征包括： 趋势：趋势是时间序列在某一方向上持续运动（比如牛市时股市每天都在上涨，股票收益率持续为正；熊市时股市每天都在下跌，股票收益率持续为负）。趋势经常出现在金融时间序列中，特别是大宗商品价格；许多商品交易顾问（CTA）基金在他们的交易算法中都使用了复杂的趋势识别模型。 季节变化：许多时间序列中包含季节变化。在金融领域，我们经常看到商品价格的季节性变化，特别是那些与生长季节或温度变化有关的商品，比如天然气。 序列相关性：金融时间序列的一个最重要特征是序列相关性（serial correlation），又称为自相关性（autocorrelation）。以投资品的收益率序列为例，我们会经常观察到一段时间内的收益率之间存在正相关或者负相关。 随机噪声：它是时间序列中除去趋势、季节变化和自相关性之后的剩余随机扰动。由于时间序列存在不确定性，随机噪声总是夹杂在时间序列中，致使时间序列表现出某种震荡式的无规律运动。 ==金融时间序列分析的核心就是挖掘该时间序列中的自相关性。== 2. 协方差和相关系数 总体协方差 \[ \mathrm{Cov}(X,Y)=\mathrm{E}[(X-\mu_{X})(Y-\mu_{Y})] \] 样本协方差 \[ \hat{\mathrm{Cov}}(X,Y)=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y}) \] 总体相关系数 \[ \rho(X,Y)=\frac{\mathrm{E}[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X\sigma_Y}=\frac{\mathrm{Cov}(X,Y)}{\sigma_X\sigma_Y} \] 样本相关系数 \[ \hat{\rho}(X,Y)=\frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{\sqrt{\sum_{i=1}^{n}(X_i-\bar{X})^2(Y_i-\bar{Y})^2}} \] 3. 时间序列的平稳性 严平稳(strictly stationary) 时间序列中任意给定长度的两段子序列都满足相同的联合分布。 这在实际中几乎不可能满足。因此，实际中我们更关注于弱平稳的时间序列。 弱平稳(weekly stationary) 如果一个时间序列 \(\{r_t\}​\) 满足以下两个条件，则它是弱平稳的： 1. 对于所有的时刻 \(t\)，有 \(\mathrm{E}[r_t]=\mu\)，其中 \(\mu\) 是一个常数。 2. 对于所有的时刻 \(t\) 和任意的间隔 \(k\)，\(r_t\) 和 \(r_{t-k}\) 的协方差 \(\sigma(r_t, r_{t-k}) = \gamma_k\)，其中 \(\gamma_k\) 与时间 \(t\) 无关，它仅仅依赖于间隔 \(k\)。特别的，当 \(k=0\) 时，这个特性意味着 \(\sigma(r_t,r_t)\) 即 \(r_t\) 的方差不随时间变化，等于一个与时间 \(t\) 无关的常数 \(\gamma_0\)，这称为方差平稳性(stationary in variance)。 ==弱平稳对于时间序列分析特别重要!==]]></content>
  </entry>
</search>
